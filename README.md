# Columbia University - Fall23 - High Performance Machine Learning (COMS6998)

**Project** : Recursive Knowledge Distillation for BERT and ViT : Successive and Multi-Agent Approaches

**Contributors** : Martin Alexandre / Elie Dufeu

## Abstract : 

This project explores the effectiveness of innovative distillation methodologies on BERT (Bidirectional Encoder Representations from Transformers) and ViT (Vision Transformer) models. The goal is to test and explore how successive knowledge distillation and multi-agent knowledge distillation can contribute to efficient model scaling while reducing computational resources.

## Results : 

The project demonstrates that successive knowledge distillation is highly effective for both BERT and ViT models. This approach not only reduces model size but also ensures the retention of critical model knowledge.
